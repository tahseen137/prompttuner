# âœ¨ PromptTuner

**Optimize your prompts, maximize your results.**

PromptTuner uses AI to analyze and improve your LLM prompts. Reduce hallucinations, improve output quality, and save up to 40% on tokens with smart prompt optimization.

![Screenshot](screenshot.png)

## Features

- ğŸ“ **Paste & Optimize** â€” Copy your existing prompt, get an improved version instantly
- ğŸ”¬ **AI Analysis** â€” Analyzes for clarity, specificity, and efficiency
- âœ¨ **Detailed Notes** â€” Understand exactly what was improved and why
- ğŸ¯ **Token Optimization** â€” Reduce costs while maintaining quality
- ğŸ“Š **Side-by-Side Compare** â€” See before and after results

## Tech Stack

- **Framework:** Next.js 14
- **Language:** TypeScript
- **Styling:** Tailwind CSS
- **Deployment:** Vercel

## Getting Started

```bash
# Clone the repository
git clone https://github.com/yourusername/prompttuner.git
cd prompttuner

# Install dependencies
npm install

# Run development server
npm run dev

# Build for production
npm run build
```

## Deployment

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/yourusername/prompttuner)

## Live Demo

ğŸ”— [prompttuner.vercel.app](https://prompttuner.vercel.app)

## License

MIT
